{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions for Inference in a Jupyter Notebook\n",
    "\n",
    "from typing import Literal, Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import uncertainty_baselines as ub\n",
    "from tqdm import tqdm\n",
    "from src.cifar.wide_resnet_factors import wide_resnet\n",
    "from src.cifar.label_corrupted_dataset import make_label_corrupted_dataset\n",
    "\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_checkpoint(model, checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Restores the model weights from the specified checkpoint directory.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The model to restore.\n",
    "        checkpoint_dir (str): Directory containing the saved checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "        print(f\"Model weights restored from {latest_checkpoint}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found in {checkpoint_dir}\")\n",
    "   \n",
    "# Load and Preprocess CIFAR-10/100 Dataset\n",
    "def load_dataset(\n",
    "    dataset_name: Literal['cifar10', 'cifar100'],\n",
    "    batch_size=32, \n",
    "    data_dir=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses CIFAR-10/100 datasets.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): The dataset to load, e.g., 'cifar10' or 'cifar100'.\n",
    "        split (str): Dataset split to load, e.g., 'train', 'test', or 'validation'.\n",
    "        batch_size (int): Batch size for loading the data.\n",
    "        data_dir (str): Directory for dataset storage.\n",
    "        corruption_type (str): For corrupted datasets, specify the corruption type.\n",
    "        severity (int): For corrupted datasets, specify the corruption severity (1-5).\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset: A preprocessed dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_test_builder = ub.datasets.get(\n",
    "        dataset_name,\n",
    "        data_dir=data_dir,\n",
    "        split=tfds.Split.TEST\n",
    "    )\n",
    "    dataset = clean_test_builder.load(batch_size=batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Build the Wide ResNet Model\n",
    "def build_wide_resnet(input_shape=(32, 32, 3), num_classes=10, depth=28, width_multiplier=2, l2=0.0):\n",
    "    \"\"\"\n",
    "    Builds a Wide ResNet model.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Input shape of the data.\n",
    "        num_classes (int): Number of output classes.\n",
    "        depth (int): Depth of the ResNet.\n",
    "        width_multiplier (int): Width multiplier for the ResNet.\n",
    "        l2 (float): L2 regularization parameter.\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: A compiled Wide ResNet model.\n",
    "    \"\"\"\n",
    "    model = wide_resnet(\n",
    "        input_shape=input_shape,\n",
    "        depth=depth,\n",
    "        width_multiplier=width_multiplier,\n",
    "        num_classes=num_classes,\n",
    "        l2=l2,\n",
    "        version=2,\n",
    "        num_factors=1,\n",
    "        no_scale=False\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_ub_wide_resnet(input_shape=(32, 32, 3), \n",
    "                         depth=28, \n",
    "                         width_multiplier=2, \n",
    "                         num_classes=10, \n",
    "                         l2=0.0, \n",
    "                         hps=None, \n",
    "                         seed=None):\n",
    "    \"\"\"\n",
    "    Build a Wide ResNet model using `ub.models.wide_resnet`.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input images.\n",
    "        depth (int): Depth of the Wide ResNet.\n",
    "        width_multiplier (int): Width multiplier for the ResNet.\n",
    "        num_classes (int): Number of output classes.\n",
    "        l2 (float): L2 regularization factor.\n",
    "        hps (dict): Additional hyperparameters.\n",
    "        seed (int): Random seed for model initialization.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A Wide ResNet model instance.\n",
    "    \"\"\"\n",
    "    model = ub.models.wide_resnet(\n",
    "        input_shape=input_shape,\n",
    "        depth=depth,\n",
    "        width_multiplier=width_multiplier,\n",
    "        num_classes=num_classes,\n",
    "        l2=l2,\n",
    "        hps=hps,\n",
    "        seed=seed\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def perform_inference(model, dataset, deterministic=False):\n",
    "    \"\"\"\n",
    "    Performs inference on the dataset using the given model.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained model.\n",
    "        dataset (tf.data.Dataset): The dataset to perform inference on.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: A list of predictions and true labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not deterministic:\n",
    "        locs, scales, all_labels = [], [], []\n",
    "        for batch in tqdm(dataset):\n",
    "            images = batch['features']\n",
    "            labels = batch['labels']\n",
    "            # Perform a forward pass\n",
    "            loc, scale = model(images, training=False)  # Adjust based on your model's output\n",
    "            locs.append(loc.numpy())\n",
    "            scales.append(scale.numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "\n",
    "        # Combine all batches into single arrays\n",
    "        locs = tf.concat(locs, axis=0).numpy()\n",
    "        scales = tf.concat(scales, axis=0).numpy()\n",
    "        all_labels = tf.concat(all_labels, axis=0).numpy()\n",
    "\n",
    "        return locs, scales, all_labels\n",
    "\n",
    "    locs, all_labels = [], []\n",
    "    for batch in tqdm(dataset):\n",
    "        images = batch['features']\n",
    "        labels = batch['labels']\n",
    "        # Perform a forward pass\n",
    "        loc = model(images, training=False)  # Adjust based on your model's output\n",
    "        locs.append(loc.numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "    # Combine all batches into single arrays\n",
    "    locs = tf.concat(locs, axis=0).numpy()\n",
    "    all_labels = tf.concat(all_labels, axis=0).numpy()\n",
    "\n",
    "    return locs, all_labels\n",
    "\n",
    "\n",
    "# Utility to Display Results\n",
    "def display_results(predictions, class_names=None):\n",
    "    \"\"\"\n",
    "    Displays predictions and their true labels.\n",
    "\n",
    "    Args:\n",
    "        predictions (List[Dict]): A list of predictions and true labels.\n",
    "        class_names (List[str]): Optional list of class names for better readability.\n",
    "    \"\"\"\n",
    "    for i, result in enumerate(predictions[:5]):  # Display first 5 examples\n",
    "        pred = result['predicted']\n",
    "        true = result['true']\n",
    "        if class_names:\n",
    "            print(f\"Example {i+1}: Predicted={class_names[pred]}, True={class_names[true]}\")\n",
    "        else:\n",
    "            print(f\"Example {i+1}: Predicted={pred}, True={true}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sgn(\n",
    "    dataset_name: str,\n",
    "    checkpoint_dir: str,\n",
    "    num_classes: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    batch_size = 32\n",
    "\n",
    "    dataset = load_dataset(dataset_name=dataset_name, batch_size=batch_size)\n",
    "\n",
    "    model = build_wide_resnet(num_classes=num_classes-1)\n",
    "    load_model_checkpoint(model, checkpoint_dir)\n",
    "\n",
    "    locs, scales, labels = perform_inference(model, dataset, deterministic=False)\n",
    "    return locs, scales, labels\n",
    "\n",
    "def evaluate_ls(\n",
    "    dataset_name: str,\n",
    "    checkpoint_dir: str,\n",
    "    num_classes: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Main Execution\n",
    "    batch_size = 32\n",
    "\n",
    "    dataset = load_dataset(dataset_name=dataset_name, batch_size=batch_size)\n",
    "\n",
    "    model = build_ub_wide_resnet(num_classes=num_classes, seed=42)\n",
    "    load_model_checkpoint(model, checkpoint_dir)\n",
    "\n",
    "    locs, labels = perform_inference(model, dataset, deterministic=True)\n",
    "    return locs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights restored from /home/baumana1/work/data/sgn_results_wrong/cifar10ls/no_noise/checkpoint-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 114/313 [00:41<01:21,  2.45it/s]"
     ]
    }
   ],
   "source": [
    "locs_ls, labels_ls = evaluate_ls(\n",
    "    dataset_name='cifar10', \n",
    "    checkpoint_dir='/home/baumana1/work/data/sgn_results_wrong/cifar10ls/no_noise',\n",
    "    num_classes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_sgn, scales_sgn, labels_sgn = evaluate_sgn(\n",
    "    dataset_name='cifar10', \n",
    "    checkpoint_dir='/home/baumana1/work/data/sgn_results_wrong/cifar10sgn/no_noise',\n",
    "    num_classes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "  0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights restored from /home/baumana1/work/data/sgn_results_wrong/cifar10ls/no_noise/checkpoint-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:32<00:00,  3.86it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10), (10000,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(locs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (preds == labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9124"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_result: [[-2.8670713e-01 -5.8261782e-01]\n",
      " [ 1.1100184e-08 -1.6978569e+00]]\n",
      "backtr_result: [[0.2 0.3 0.5]\n",
      " [0.1 0.1 0.8]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# CLR inverse function\n",
    "def clr_inv(p):\n",
    "    z = tf.math.log(p)\n",
    "    return z - tf.reduce_mean(z, axis=1, keepdims=True)\n",
    "\n",
    "# CLR forward function\n",
    "def clr_forward(z, axis=1):\n",
    "    return tf.nn.softmax(z, axis=axis)\n",
    "\n",
    "def helmert_tf(n):\n",
    "  tensor = tf.ones((n, n))\n",
    "  H = tf.linalg.set_diag(tf.linalg.band_part(tensor, -1, 0), 1-tf.range(1, n+1, dtype=tf.float32))\n",
    "  d = tf.range(0, n, dtype=tf.float32) * tf.range(1, n+1, dtype=tf.float32)\n",
    "  H_full = H / tf.math.sqrt(d)[:, tf.newaxis]\n",
    "  return H_full[1:]\n",
    "\n",
    "\n",
    "def ilr_forward(z, axis=-1):\n",
    "    H = helmert_tf(tf.shape(z)[-1] + 1)\n",
    "    return clr_forward(z @ H, axis=axis)\n",
    "\n",
    "\n",
    "def ilr_inv(p):\n",
    "    z = clr_inv(p)\n",
    "    H = helmert_tf(tf.shape(p)[-1])\n",
    "    return z @ tf.linalg.matrix_transpose(H)\n",
    "\n",
    "# Test the CLR functions\n",
    "test_input = tf.constant([[0.2, 0.3, 0.5], [0.1, 0.1, 0.8]], dtype=tf.float32)\n",
    "\n",
    "# Apply forward and inverse transformations\n",
    "gaussian_result = ilr_inv(test_input)\n",
    "backtr_result = ilr_forward(gaussian_result)\n",
    "\n",
    "print(\"gaussian_result:\", gaussian_result.numpy())\n",
    "print(\"backtr_result:\", backtr_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
